{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Preprocessing from package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports packages and sample .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking virtualenv with `pyenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pyenv virtualenvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Python 3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordListCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d6f36c68f266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'WordListCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "list(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Select the data_samples to read .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run only once, multiple runs will give an error =>restart kernel\n",
    "print(os.getcwd())\n",
    "print(os.path.join('.','data_sample', 'plain_text_sample'))\n",
    "os.chdir(os.path.join('..', 'IIPE', 'data_sample', 'plain_text_sample'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir()\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean file names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean file names, return ref and date YYYY-MM-DD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(lst):\n",
    "    \"\"\"returns a list of tuples<reference, date>\"\"\"\n",
    "    cleaned = [name.replace('Reports_Plain text_','').replace('.txt','') for name in os.listdir() if name.endswith('.txt')]\n",
    "    splitted = [name.split('_') for name in cleaned]\n",
    "    references = [lst[0] for lst in splitted]\n",
    "    dates = ['-'.join(name[1:][::-1]) for name in splitted]\n",
    "    return [(r,d) for r, d in zip(references, dates)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_file_names(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3"
   },
   "source": [
    "## Cleaning unusefull content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read `.txt`files into a pandas.Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_contents_df(lst):\n",
    "    \"\"\"Returns a dataframe with date, reference, text from a list of file_names\"\"\"\n",
    "    #init a list of dictionnaries\n",
    "    ld_contents = []\n",
    "    \n",
    "    for file in lst:\n",
    "        if file.endswith('.txt'):\n",
    "            # keeping the reference and the date\n",
    "            split = file.replace('Reports_Plain text_','').replace('.txt','').split('_')\n",
    "            reference = split[0]\n",
    "            date= '-'.join(split[1:][::-1])\n",
    "\n",
    "            #creating the dictionnary\n",
    "            d = {'date':date,\n",
    "                 'reference':reference,\n",
    "                 'text': '' }\n",
    "\n",
    "            #adding text content to the dictionary\n",
    "            with open(file, encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                d['text']=text\n",
    "            ld_contents.append(d)\n",
    "    #create dataframe and set date to a datetime datatype\n",
    "\n",
    "    df_contents = pd.DataFrame(ld_contents)\n",
    "    df_contents['date'] = pd.to_datetime(df_contents['date'])\n",
    "    return df_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FilesProperlyConverted = make_contents_df(file_names)\n",
    "df_FilesProperlyConverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3"
   },
   "source": [
    "### Most used words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python 3"
   },
   "outputs": [],
   "source": [
    "def make_tokens(df):\n",
    "    \"\"\"Removes stopwords, stems and lemmatizes\n",
    "    Returns clean tokens\"\"\"\n",
    "    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    #turns the text in the dataframe into a long list of words\n",
    "    TotalText = []\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        TotalText.append(text)\n",
    "    \n",
    "    #stopwords, with plurals (otherwise the lemmatizong steps puts some of the stopwords back)\n",
    "    newStopWords = ['school','learning','student','pupil','teacher','management','teaching','support', 'lesson', 'board']\n",
    "    newStopWords_plur = ['schools','learnings','students','pupils','teachers','managements','teachings','supports', 'lessons', 'boards']\n",
    "    newStopWords += newStopWords_plur\n",
    "    stopwords = stopwords.union(newStopWords)\n",
    "    TotalText = \" \".join(TotalText)\n",
    "    \n",
    "    #tokenization\n",
    "    tokens = [w for w in word_tokenize(TotalText.lower()) if w.isalpha()]          # isalpha() checks if each word is alphabetical, lower() transforms everything to lowercase\n",
    "    no_stop = [t.strip() for t in tokens if t.strip() not in stopwords]      # stopwords already comes with a built-in list of words to remove\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stop]\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "#turns the text in the dataframe into a long list of words\n",
    "TotalText = []\n",
    "for index, row in df_FilesProperlyConverted.iterrows():\n",
    "    text = row['text']\n",
    "    TotalText.append(text)\n",
    "len(TotalText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalText = list(df_FilesProperlyConverted.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords, with plurals (otherwise the lemmatizong steps puts some of the stopwords back)\n",
    "#newStopWords = ['school','learning','student','pupil','teacher','management','teaching','support', 'lesson', 'board']\n",
    "#newStopWords_plur = ['schools','learnings','students','pupils','teachers','managements','teachings','supports', 'lessons', 'boards']\n",
    "newStopWords += newStopWords_plur\n",
    "stopwords = stopwords.union(newStopWords)\n",
    "TotalText = \" \".join(TotalText)\n",
    "\n",
    "#tokenization\n",
    "tokens = [w for w in word_tokenize(TotalText.lower()) if w.isalpha()]          # isalpha() checks if each word is alphabetical, lower() transforms everything to lowercase\n",
    "no_stop = [t.strip() for t in tokens if t.strip() not in stopwords]      # stopwords already comes with a built-in list of words to remove\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Python 3"
   },
   "outputs": [],
   "source": [
    "bow = Counter(lemmatized)\n",
    "MostCommon = dict(bow.most_common(10))\n",
    "\n",
    "plt.bar(*zip(*MostCommon.items()))\n",
    "plt.title('Whole sample')\n",
    "plt.xlabel('Most common words')\n",
    "plt.ylabel('Number of times the word appears')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.savefig(\"Results\\\\Word count\\\\Whole sample.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python 3"
   },
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "sos": {
   "kernels": [
    [
     "Python 3",
     "python3",
     "python3",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.21.20"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
